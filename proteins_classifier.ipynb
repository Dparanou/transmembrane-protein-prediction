{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef, balanced_accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('proteins_dataframe.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['residue_number', 'res_index', 'Hydrophobicity', 'Hydrophilicity', 'NCI', 'Polarity', 'Polarizability', 'SASA', 'V', 'is_charged']]\n",
    "y = df['tm_segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>residue_number</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>Hydrophilicity</th>\n",
       "      <th>NCI</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Polarizability</th>\n",
       "      <th>SASA</th>\n",
       "      <th>V</th>\n",
       "      <th>is_charged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.268115</td>\n",
       "      <td>-1.235368</td>\n",
       "      <td>2.046999</td>\n",
       "      <td>-1.111856</td>\n",
       "      <td>2.103068</td>\n",
       "      <td>-0.561175</td>\n",
       "      <td>-0.316707</td>\n",
       "      <td>-0.789434</td>\n",
       "      <td>2.268825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.037428</td>\n",
       "      <td>0.827785</td>\n",
       "      <td>-0.777668</td>\n",
       "      <td>0.109571</td>\n",
       "      <td>-1.166392</td>\n",
       "      <td>0.308144</td>\n",
       "      <td>0.440488</td>\n",
       "      <td>0.677903</td>\n",
       "      <td>-0.440757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037428</td>\n",
       "      <td>0.827785</td>\n",
       "      <td>-0.777668</td>\n",
       "      <td>0.109571</td>\n",
       "      <td>-1.166392</td>\n",
       "      <td>0.308144</td>\n",
       "      <td>0.440488</td>\n",
       "      <td>0.677903</td>\n",
       "      <td>-0.440757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.314196</td>\n",
       "      <td>-1.109052</td>\n",
       "      <td>0.399277</td>\n",
       "      <td>-0.639219</td>\n",
       "      <td>1.537976</td>\n",
       "      <td>-0.249938</td>\n",
       "      <td>-0.167029</td>\n",
       "      <td>-0.276552</td>\n",
       "      <td>-0.440757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.268115</td>\n",
       "      <td>-1.235368</td>\n",
       "      <td>2.046999</td>\n",
       "      <td>-1.111856</td>\n",
       "      <td>2.103068</td>\n",
       "      <td>-0.561175</td>\n",
       "      <td>-0.316707</td>\n",
       "      <td>-0.789434</td>\n",
       "      <td>2.268825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138180</th>\n",
       "      <td>1.369070</td>\n",
       "      <td>0.848838</td>\n",
       "      <td>-0.601126</td>\n",
       "      <td>0.195840</td>\n",
       "      <td>-0.762755</td>\n",
       "      <td>-0.185544</td>\n",
       "      <td>-0.189040</td>\n",
       "      <td>0.074512</td>\n",
       "      <td>-0.440757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138181</th>\n",
       "      <td>-0.213241</td>\n",
       "      <td>-1.866945</td>\n",
       "      <td>2.046999</td>\n",
       "      <td>-0.439951</td>\n",
       "      <td>1.416885</td>\n",
       "      <td>0.662310</td>\n",
       "      <td>1.160264</td>\n",
       "      <td>0.856178</td>\n",
       "      <td>2.268825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138182</th>\n",
       "      <td>-0.213241</td>\n",
       "      <td>-1.866945</td>\n",
       "      <td>2.046999</td>\n",
       "      <td>-0.439951</td>\n",
       "      <td>1.416885</td>\n",
       "      <td>0.662310</td>\n",
       "      <td>1.160264</td>\n",
       "      <td>0.856178</td>\n",
       "      <td>2.268825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138183</th>\n",
       "      <td>-0.213241</td>\n",
       "      <td>-1.866945</td>\n",
       "      <td>2.046999</td>\n",
       "      <td>-0.439951</td>\n",
       "      <td>1.416885</td>\n",
       "      <td>0.662310</td>\n",
       "      <td>1.160264</td>\n",
       "      <td>0.856178</td>\n",
       "      <td>2.268825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138184</th>\n",
       "      <td>0.841633</td>\n",
       "      <td>-2.951154</td>\n",
       "      <td>2.046999</td>\n",
       "      <td>-0.021241</td>\n",
       "      <td>1.093975</td>\n",
       "      <td>1.435038</td>\n",
       "      <td>1.825010</td>\n",
       "      <td>0.993312</td>\n",
       "      <td>2.268825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138185 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        residue_number  Hydrophobicity  Hydrophilicity       NCI  Polarity  \\\n",
       "0            -1.268115       -1.235368        2.046999 -1.111856  2.103068   \n",
       "1            -0.037428        0.827785       -0.777668  0.109571 -1.166392   \n",
       "2            -0.037428        0.827785       -0.777668  0.109571 -1.166392   \n",
       "3             0.314196       -1.109052        0.399277 -0.639219  1.537976   \n",
       "4            -1.268115       -1.235368        2.046999 -1.111856  2.103068   \n",
       "...                ...             ...             ...       ...       ...   \n",
       "138180        1.369070        0.848838       -0.601126  0.195840 -0.762755   \n",
       "138181       -0.213241       -1.866945        2.046999 -0.439951  1.416885   \n",
       "138182       -0.213241       -1.866945        2.046999 -0.439951  1.416885   \n",
       "138183       -0.213241       -1.866945        2.046999 -0.439951  1.416885   \n",
       "138184        0.841633       -2.951154        2.046999 -0.021241  1.093975   \n",
       "\n",
       "        Polarizability      SASA         V  is_charged  \n",
       "0            -0.561175 -0.316707 -0.789434    2.268825  \n",
       "1             0.308144  0.440488  0.677903   -0.440757  \n",
       "2             0.308144  0.440488  0.677903   -0.440757  \n",
       "3            -0.249938 -0.167029 -0.276552   -0.440757  \n",
       "4            -0.561175 -0.316707 -0.789434    2.268825  \n",
       "...                ...       ...       ...         ...  \n",
       "138180       -0.185544 -0.189040  0.074512   -0.440757  \n",
       "138181        0.662310  1.160264  0.856178    2.268825  \n",
       "138182        0.662310  1.160264  0.856178    2.268825  \n",
       "138183        0.662310  1.160264  0.856178    2.268825  \n",
       "138184        1.435038  1.825010  0.993312    2.268825  \n",
       "\n",
       "[138185 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data normilizations\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaled, columns=X.columns)\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False) # 7 indicates that always get the same split of data each time this example is executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:53:43] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\teval-auc:0.66145\ttrain-auc:0.70841\n",
      "[1]\teval-auc:0.66172\ttrain-auc:0.70840\n",
      "[2]\teval-auc:0.66195\ttrain-auc:0.70871\n",
      "[3]\teval-auc:0.66186\ttrain-auc:0.70887\n",
      "[4]\teval-auc:0.66225\ttrain-auc:0.70931\n",
      "[5]\teval-auc:0.66256\ttrain-auc:0.70952\n",
      "[6]\teval-auc:0.66260\ttrain-auc:0.70968\n",
      "[7]\teval-auc:0.66304\ttrain-auc:0.71011\n",
      "[8]\teval-auc:0.66333\ttrain-auc:0.71025\n",
      "[9]\teval-auc:0.66367\ttrain-auc:0.71042\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# xgboost parameters\n",
    "param = {'max_depth': 12, 'eta': 1, 'objective': 'binary:logistic', 'nthread': 4, 'eval_metric': 'auc', 'n_estimators': 100, 'learning_rate': 0.01}\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "num_round = 10\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 62.46%\n",
      "Roc: 66.60%\n",
      "F1 score: 61.11%\n"
     ]
    }
   ],
   "source": [
    "# fit model on training data\n",
    "model = XGBClassifier(objective=\"binary:logistic\", n_estimators = 100, learning_rate = 0.01, max_depth = 12)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "print(\"Balanced accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "roc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print(\"Roc: %.2f%%\" % (roc_score* 100.0))\n",
    "\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"F1 score: %.2f%%\" % (f1* 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_metric = 'balanced_accuracy_score'\n",
    "outer_metrics = ['balanced_accuracy_score', 'average_precision', 'f1']\n",
    "\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv_inner = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "cv_outer = StratifiedKFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.6129532102797542\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.6129532386523935\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.6129532044164002\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.612953192615667\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.6129532610790335\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.6129532122198383\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.612953238518026\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.6129532072701179\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.6129531919977071\n",
      "Best parameters:  {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 10}\n",
      "Best score:  0.612953244873593\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(objective=\"binary:logistic\")\n",
    "\n",
    "param_grid = {\n",
    "  \"max_depth\": [1, 3, 7, 10],\n",
    "  \"n_estimators\": [10, 500, 1000],\n",
    "  \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# define search\n",
    "rf_grid = GridSearchCV(model, param_grid, scoring='balanced_accuracy', n_jobs=-1, cv=cv_inner, refit=True)\n",
    "\n",
    "# execute the nested cross-validation\n",
    "scores = cross_validate(rf_grid, scaled_df, y, scoring='balanced_accuracy', cv=cv_outer, n_jobs=-1)\n",
    "\n",
    "#Best\n",
    "rf_grid.fit(X, y)\n",
    "# params[r] = rf_grid.best_params_\n",
    "print(\"Best parameters: \", rf_grid.best_params_)\n",
    "\n",
    "# scores[r] = rf_grid.best_score_\n",
    "print(\"Best score: \", rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathwe cor is : 23.79%\n",
      "Accuracy cor is : 61.91%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "ypred = svm.predict(X_test)\n",
    "\n",
    "print('Mathwe cor is : %.2f%%' % ( matthews_corrcoef(y_test.values, ypred) * 100.0))\n",
    "print('Accuracy cor is : %.2f%%' % (balanced_accuracy_score(y_test.values, ypred) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dparanou/.local/lib/python3.8/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/dparanou/.local/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\ttraining's auc: 0.702665\ttraining's binary_logloss: 0.616961\ttraining's average_precision: 0.621857\tvalid_0's auc: 0.675617\tvalid_0's binary_logloss: 0.639261\tvalid_0's average_precision: 0.604244\n",
      "[40]\ttraining's auc: 0.710493\ttraining's binary_logloss: 0.611079\ttraining's average_precision: 0.632799\tvalid_0's auc: 0.671358\tvalid_0's binary_logloss: 0.643621\tvalid_0's average_precision: 0.599755\n",
      "[60]\ttraining's auc: 0.71549\ttraining's binary_logloss: 0.607267\ttraining's average_precision: 0.639198\tvalid_0's auc: 0.667483\tvalid_0's binary_logloss: 0.646858\tvalid_0's average_precision: 0.595502\n",
      "[80]\ttraining's auc: 0.71945\ttraining's binary_logloss: 0.604323\ttraining's average_precision: 0.64388\tvalid_0's auc: 0.66522\tvalid_0's binary_logloss: 0.649225\tvalid_0's average_precision: 0.59272\n",
      "[100]\ttraining's auc: 0.72254\ttraining's binary_logloss: 0.602057\ttraining's average_precision: 0.647789\tvalid_0's auc: 0.66303\tvalid_0's binary_logloss: 0.651247\tvalid_0's average_precision: 0.590652\n",
      "[120]\ttraining's auc: 0.724718\ttraining's binary_logloss: 0.600271\ttraining's average_precision: 0.650854\tvalid_0's auc: 0.661442\tvalid_0's binary_logloss: 0.653065\tvalid_0's average_precision: 0.588417\n",
      "[140]\ttraining's auc: 0.726429\ttraining's binary_logloss: 0.598837\ttraining's average_precision: 0.652886\tvalid_0's auc: 0.660173\tvalid_0's binary_logloss: 0.654513\tvalid_0's average_precision: 0.586894\n",
      "[160]\ttraining's auc: 0.727562\ttraining's binary_logloss: 0.597792\ttraining's average_precision: 0.654485\tvalid_0's auc: 0.658486\tvalid_0's binary_logloss: 0.656443\tvalid_0's average_precision: 0.584878\n",
      "[180]\ttraining's auc: 0.728574\ttraining's binary_logloss: 0.596912\ttraining's average_precision: 0.655762\tvalid_0's auc: 0.657286\tvalid_0's binary_logloss: 0.657828\tvalid_0's average_precision: 0.583629\n",
      "[200]\ttraining's auc: 0.729325\ttraining's binary_logloss: 0.596153\ttraining's average_precision: 0.656721\tvalid_0's auc: 0.656215\tvalid_0's binary_logloss: 0.659038\tvalid_0's average_precision: 0.582406\n",
      "[220]\ttraining's auc: 0.729875\ttraining's binary_logloss: 0.595608\ttraining's average_precision: 0.657436\tvalid_0's auc: 0.654996\tvalid_0's binary_logloss: 0.660436\tvalid_0's average_precision: 0.580804\n",
      "[240]\ttraining's auc: 0.730373\ttraining's binary_logloss: 0.595144\ttraining's average_precision: 0.658211\tvalid_0's auc: 0.653865\tvalid_0's binary_logloss: 0.661718\tvalid_0's average_precision: 0.579615\n",
      "[260]\ttraining's auc: 0.730776\ttraining's binary_logloss: 0.594701\ttraining's average_precision: 0.658779\tvalid_0's auc: 0.652885\tvalid_0's binary_logloss: 0.662747\tvalid_0's average_precision: 0.578514\n",
      "[280]\ttraining's auc: 0.731164\ttraining's binary_logloss: 0.594299\ttraining's average_precision: 0.659438\tvalid_0's auc: 0.652429\tvalid_0's binary_logloss: 0.663594\tvalid_0's average_precision: 0.578061\n",
      "[300]\ttraining's auc: 0.731399\ttraining's binary_logloss: 0.59402\ttraining's average_precision: 0.659785\tvalid_0's auc: 0.652035\tvalid_0's binary_logloss: 0.664269\tvalid_0's average_precision: 0.577661\n",
      "[320]\ttraining's auc: 0.73166\ttraining's binary_logloss: 0.593725\ttraining's average_precision: 0.660192\tvalid_0's auc: 0.651444\tvalid_0's binary_logloss: 0.665001\tvalid_0's average_precision: 0.576987\n",
      "[340]\ttraining's auc: 0.731795\ttraining's binary_logloss: 0.593526\ttraining's average_precision: 0.660415\tvalid_0's auc: 0.651052\tvalid_0's binary_logloss: 0.665672\tvalid_0's average_precision: 0.576406\n",
      "[360]\ttraining's auc: 0.73191\ttraining's binary_logloss: 0.593377\ttraining's average_precision: 0.660559\tvalid_0's auc: 0.650706\tvalid_0's binary_logloss: 0.666194\tvalid_0's average_precision: 0.576082\n",
      "[380]\ttraining's auc: 0.732051\ttraining's binary_logloss: 0.593193\ttraining's average_precision: 0.66078\tvalid_0's auc: 0.650317\tvalid_0's binary_logloss: 0.666793\tvalid_0's average_precision: 0.575536\n",
      "[400]\ttraining's auc: 0.732152\ttraining's binary_logloss: 0.593059\ttraining's average_precision: 0.660968\tvalid_0's auc: 0.650035\tvalid_0's binary_logloss: 0.667412\tvalid_0's average_precision: 0.575238\n",
      "[420]\ttraining's auc: 0.73224\ttraining's binary_logloss: 0.59293\ttraining's average_precision: 0.661146\tvalid_0's auc: 0.649866\tvalid_0's binary_logloss: 0.667848\tvalid_0's average_precision: 0.57502\n",
      "[440]\ttraining's auc: 0.732311\ttraining's binary_logloss: 0.592824\ttraining's average_precision: 0.661265\tvalid_0's auc: 0.649575\tvalid_0's binary_logloss: 0.668172\tvalid_0's average_precision: 0.574709\n",
      "[460]\ttraining's auc: 0.732379\ttraining's binary_logloss: 0.592716\ttraining's average_precision: 0.661386\tvalid_0's auc: 0.649357\tvalid_0's binary_logloss: 0.668621\tvalid_0's average_precision: 0.574346\n",
      "[480]\ttraining's auc: 0.732444\ttraining's binary_logloss: 0.592621\ttraining's average_precision: 0.661482\tvalid_0's auc: 0.649059\tvalid_0's binary_logloss: 0.669048\tvalid_0's average_precision: 0.573934\n",
      "[500]\ttraining's auc: 0.732494\ttraining's binary_logloss: 0.592543\ttraining's average_precision: 0.661561\tvalid_0's auc: 0.648859\tvalid_0's binary_logloss: 0.669373\tvalid_0's average_precision: 0.573668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.3, max_depth=12, num_iterations=500,\n",
       "               num_leaves=70, objective='binary')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "# param = {'num_leaves': 70, 'objective': 'binary'}\n",
    "# param['metric'] = 'auc'\n",
    "\n",
    "model = lgb.LGBMClassifier(learning_rate=0.3, max_depth=12, num_leaves=70, num_iterations=500, objective='binary')\n",
    "\n",
    "# num_round = 10\n",
    "# lgb.cv(param, train_data, num_round, nfold=5)\n",
    "model.fit(X_train,y_train, eval_set=[(X_test,y_test),(X_train,y_train)], verbose=20, eval_metric=['auc', 'logloss', 'average_precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.6660\n",
      "Testing accuracy 0.6098\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy {:.4f}'.format(model.score(X_train,y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
